{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Include this at the top of your colab code\n",
    "import os\n",
    "\n",
    "if not os.path.exists(\".mujoco_setup_complete\"):\n",
    "    # Get the prereqs\n",
    "    !apt-get -qq update\n",
    "    !apt-get -qq install -y libosmesa6-dev libgl1-mesa-glx libglfw3 libgl1-mesa-dev libglew-dev patchelf\n",
    "    # Get Mujoco\n",
    "    !mkdir ~/.mujoco\n",
    "    !wget -q https://mujoco.org/download/mujoco210-linux-x86_64.tar.gz -O mujoco.tar.gz\n",
    "    !tar -zxf mujoco.tar.gz -C \"$HOME/.mujoco\"\n",
    "    !rm mujoco.tar.gz\n",
    "    # Add it to the actively loaded path and the bashrc path (these only do so much)\n",
    "    !echo 'export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/.mujoco/mujoco210/bin' >> ~/.bashrc\n",
    "    !echo 'export LD_PRELOAD=$LD_PRELOAD:/usr/lib/x86_64-linux-gnu/libGLEW.so' >> ~/.bashrc\n",
    "    # THE ANNOYING ONE, FORCE IT INTO LDCONFIG SO WE ACTUALLY GET ACCESS TO IT THIS SESSION\n",
    "    !echo \"/root/.mujoco/mujoco210/bin\" > /etc/ld.so.conf.d/mujoco_ld_lib_path.conf\n",
    "    !ldconfig\n",
    "    # Install Mujoco-py\n",
    "    !pip3 install -U 'mujoco-py<2.2,>=2.1'\n",
    "    # run once\n",
    "    !touch .mujoco_setup_complete\n",
    "\n",
    "try:\n",
    "    if _mujoco_run_once:\n",
    "        pass\n",
    "except NameError:\n",
    "    _mujoco_run_once = False\n",
    "if not _mujoco_run_once:\n",
    "    # Add it to the actively loaded path and the bashrc path (these only do so much)\n",
    "    try:\n",
    "        os.environ[\"LD_LIBRARY_PATH\"] = (\n",
    "            os.environ[\"LD_LIBRARY_PATH\"] + \":/root/.mujoco/mujoco210/bin\"\n",
    "        )\n",
    "        os.environ[\"LD_LIBRARY_PATH\"] = (\n",
    "            os.environ[\"LD_LIBRARY_PATH\"] + \":/usr/lib/nvidia\"\n",
    "        )\n",
    "    except KeyError:\n",
    "        os.environ[\"LD_LIBRARY_PATH\"] = \"/root/.mujoco/mujoco210/bin\"\n",
    "    try:\n",
    "        os.environ[\"LD_PRELOAD\"] = (\n",
    "            os.environ[\"LD_PRELOAD\"] + \":/usr/lib/x86_64-linux-gnu/libGLEW.so\"\n",
    "        )\n",
    "    except KeyError:\n",
    "        os.environ[\"LD_PRELOAD\"] = \"/usr/lib/x86_64-linux-gnu/libGLEW.so\"\n",
    "    # presetup so we don't see output on first env initialization\n",
    "    import mujoco_py\n",
    "\n",
    "    _mujoco_run_once = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/tinkoff-ai/d4rl@master#egg=d4rl"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install wandb dacite"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import yaml\n",
    "import copy\n",
    "import random\n",
    "from dacite import from_dict\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Callable\n",
    "from functools import partial\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "import gym\n",
    "import d4rl\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PreprocessingConfig:\n",
    "    normalize_reward: bool = True\n",
    "    std_eps: float = 1e-3\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    env_name: str = \"antmaze-medium-play-v0\"\n",
    "    save_path: str = \"\"\n",
    "    batch_size: int = 256\n",
    "    buffer_size: int = 2_000_000\n",
    "    max_steps: int = 1_000_000\n",
    "    seed: int = 0\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EvalConfig:\n",
    "    evaluate_every_n: int = 5_000\n",
    "    eval_episodes: int = 100\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class FineTuneConfig:\n",
    "    batch_size: int = 256\n",
    "    max_steps: int = 1_000_000\n",
    "    save_path: str = \"\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class IQLConfig:\n",
    "    alpha: float = 0.005\n",
    "    beta: float = 10.0\n",
    "    tau: float = 0.9\n",
    "    gamma: float = 0.99\n",
    "    max_weight: float = 100.0\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    project: str = \"TLab-Application-v0\"\n",
    "    group: str = \"IQL\"\n",
    "    name: str = \"IQL\"\n",
    "    preprocess: PreprocessingConfig = PreprocessingConfig()\n",
    "    eval: EvalConfig = EvalConfig()\n",
    "    train: TrainingConfig = TrainingConfig()\n",
    "    finetune: FineTuneConfig = FineTuneConfig()\n",
    "    algorithm: IQLConfig = IQLConfig()\n",
    "\n",
    "\n",
    "def seed_everything(\n",
    "    seed: int, env: gym.Env | None = None, deterministic_torch: bool = False\n",
    "):\n",
    "    if env is not None:\n",
    "        env.seed(seed)\n",
    "        env.action_space.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.use_deterministic_algorithms(deterministic_torch)\n",
    "\n",
    "\n",
    "def asymetric_loss(sample: torch.Tensor, tau: float) -> torch.Tensor:\n",
    "    loss = (tau - (sample < 0).float()).abs() * sample**2\n",
    "    return loss.mean()\n",
    "\n",
    "\n",
    "TensorBatch = list[torch.Tensor]\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        capacity: int,\n",
    "        device: str = \"cpu\",\n",
    "    ):\n",
    "        self._capacity = capacity\n",
    "        self._device = device\n",
    "        self._size = 0\n",
    "        self._ptr = 0\n",
    "\n",
    "        self._states = self._zeros_tensor(shape=(capacity, state_dim))\n",
    "        self._actions = self._zeros_tensor(shape=(capacity, action_dim))\n",
    "        self._rewards = self._zeros_tensor(shape=(capacity, 1))\n",
    "        self._next_states = self._zeros_tensor(shape=(capacity, state_dim))\n",
    "        self._dones = self._zeros_tensor(shape=(capacity, 1))\n",
    "\n",
    "    def _to_tensor(self, data: np.ndarray) -> torch.Tensor:\n",
    "        return torch.tensor(data, dtype=torch.float32, device=self._device)\n",
    "\n",
    "    def _zeros_tensor(self, shape: tuple[int, ...]) -> torch.Tensor:\n",
    "        return torch.zeros(shape, dtype=torch.float32, device=self._device)\n",
    "\n",
    "    def load_d4rl_dataset(self, data: dict[str, np.ndarray], info: bool = True) -> None:\n",
    "        if self._size != 0:\n",
    "            raise ValueError(\"Trying to load data into non-empty replay buffer\")\n",
    "        n_transitions = data[\"observations\"].shape[0]\n",
    "        if n_transitions > self._capacity:\n",
    "            raise ValueError(\n",
    "                f\"Buffer capacity is smaller than the size of the dataset: {self._capacity} < {n_transitions}\"\n",
    "            )\n",
    "\n",
    "        if info:\n",
    "            print(f\"Loading the dataset of size {n_transitions}...\")\n",
    "\n",
    "        self._states[:n_transitions] = self._to_tensor(data[\"observations\"])\n",
    "        self._actions[:n_transitions] = self._to_tensor(data[\"actions\"])\n",
    "        self._rewards[:n_transitions] = self._to_tensor(data[\"rewards\"][..., None])\n",
    "        self._next_states[:n_transitions] = self._to_tensor(data[\"next_observations\"])\n",
    "        self._dones[:n_transitions] = self._to_tensor(data[\"terminals\"][..., None])\n",
    "        self._size, self._ptr = n_transitions, n_transitions\n",
    "\n",
    "        if info:\n",
    "            print(f\"Successfuly loaded. Size: {n_transitions}\")\n",
    "\n",
    "    def sample(self, batch_size: int) -> TensorBatch:\n",
    "        inds = np.random.randint(0, self._size, size=batch_size)\n",
    "        states, actions, rewards, next_states, dones = (\n",
    "            self._states[inds],\n",
    "            self._actions[inds],\n",
    "            self._rewards[inds],\n",
    "            self._next_states[inds],\n",
    "            self._dones[inds],\n",
    "        )\n",
    "        return [states, actions, rewards, next_states, dones]\n",
    "\n",
    "    def insert(\n",
    "        self,\n",
    "        state: np.ndarray,\n",
    "        action: np.ndarray,\n",
    "        reward: float,\n",
    "        done: float,\n",
    "        next_state: np.ndarray,\n",
    "    ) -> None:\n",
    "        self._states[self._ptr] = self._to_tensor(state)\n",
    "        self._actions[self._ptr] = self._to_tensor(action)\n",
    "        self._rewards[self._ptr] = float(reward)\n",
    "        self._dones[self._ptr] = float(done)\n",
    "        self._next_states[self._ptr] = self._to_tensor(next_state)\n",
    "\n",
    "        self._ptr = (self._ptr + 1) % self._capacity\n",
    "        self._size = min(self._size + 1, self._capacity)\n",
    "\n",
    "\n",
    "class Squeeze(nn.Module):\n",
    "    def __init__(self, dim=-1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x.squeeze(dim=self.dim)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dims: tuple[int, ...],\n",
    "        activation_fn: Callable[[], nn.Module] = nn.ReLU,\n",
    "        output_fn: Callable[[], nn.Module] = None,\n",
    "        squeeze_output: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        n_dims = len(dims)\n",
    "        if n_dims < 2:\n",
    "            raise ValueError(\"MLP requires at least two dims (input and output)\")\n",
    "        layers = []\n",
    "        for i in range(n_dims - 2):\n",
    "            layers.append(nn.Linear(dims[i], dims[i + 1]))\n",
    "            layers.append(activation_fn())\n",
    "        layers.append(nn.Linear(dims[-2], dims[-1]))\n",
    "        if output_fn is not None:\n",
    "            layers.append(output_fn())\n",
    "        if squeeze_output and dims[-1] != 1:\n",
    "            raise ValueError(\"Last dim must be 1 when squeezing\")\n",
    "        if squeeze_output:\n",
    "            layers.append(Squeeze(-1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class BasePolicy(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        hidden_dim: int,\n",
    "        n_hidden: int,\n",
    "        max_action: float,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.max_action = max_action\n",
    "        self.net = MLP(\n",
    "            dims=(state_dim, *[hidden_dim for _ in range(n_hidden)], action_dim),\n",
    "            output_fn=nn.Tanh,\n",
    "        )\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        raise NotImplemented\n",
    "\n",
    "    def _extract_action(self, action: torch.Tensor) -> torch.Tensor:\n",
    "        raise NotImplemented\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def act(self, state: np.ndarray, device: str = \"cpu\"):\n",
    "        state = torch.tensor(state.reshape(1, -1), device=device, dtype=torch.float32)\n",
    "        action = self(state)\n",
    "        action = self._extract_action(action)\n",
    "        return (\n",
    "            torch.clamp(action * self.max_action, -self.max_action, self.max_action)\n",
    "            .cpu()\n",
    "            .data.numpy()\n",
    "            .flatten()\n",
    "        )\n",
    "\n",
    "\n",
    "class GaussianPolicy(BasePolicy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim: int,\n",
    "        action_dim: int,\n",
    "        max_action: float,\n",
    "        hidden_dim: int = 256,\n",
    "        n_hidden: int = 2,\n",
    "        log_std_min: float = -10.0,\n",
    "        log_std_max: float = 2.0,\n",
    "    ):\n",
    "        super().__init__(state_dim, action_dim, hidden_dim, n_hidden, max_action)\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim, dtype=torch.float32))\n",
    "        self.log_std_min = log_std_min\n",
    "        self.log_std_max = log_std_max\n",
    "\n",
    "    def _extract_action(self, action: torch.Tensor) -> torch.Tensor:\n",
    "        return action.mean if not self.training else action.sample()\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        mu = self.net(state)\n",
    "        std = torch.exp(self.log_std.clamp(self.log_std_min, self.log_std_max))\n",
    "        return MultivariateNormal(mu, scale_tril=torch.diag(std))\n",
    "\n",
    "\n",
    "class DeterministicPolicy(BasePolicy):\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        return self.nex(state)\n",
    "\n",
    "    def _extract_action(self, action: torch.Tensor) -> torch.Tensor:\n",
    "        return action\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(\n",
    "        self, state_dim: int, action_dim: int, hidden_dim: int = 256, n_hidden: int = 2\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.q1 = MLP(\n",
    "            dims=(state_dim + action_dim, *[hidden_dim for _ in range(n_hidden)], 1),\n",
    "            squeeze_output=True,\n",
    "        )\n",
    "        self.q2 = copy.deepcopy(self.q1)\n",
    "\n",
    "    def forward(self, state: torch.Tensor, action: torch.Tensor) -> tuple[torch.Tensor]:\n",
    "        sa = torch.cat([state, action], dim=1)\n",
    "        return self.q1(sa), self.q2(sa)\n",
    "\n",
    "\n",
    "class ValueCritic(nn.Module):\n",
    "    def __init__(self, state_dim: int, hidden_dim: int = 256, n_hidden: int = 2):\n",
    "        super().__init__()\n",
    "        self.value = MLP(\n",
    "            dims=(state_dim, *[hidden_dim for _ in range(n_hidden)], 1),\n",
    "            squeeze_output=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
    "        return self.value(state)\n",
    "\n",
    "\n",
    "class IQL(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "    def setup(\n",
    "        self,\n",
    "        actor: BasePolicy,\n",
    "        actor_optim_cls: torch.optim.Optimizer,\n",
    "        actor_optim_kwargs: dict,\n",
    "        actor_scheduler: torch.optim.lr_scheduler.LRScheduler,\n",
    "        critic: Critic,\n",
    "        critic_optim_cls: torch.optim.Optimizer,\n",
    "        critic_optim_kwargs: dict,\n",
    "        value_critic: ValueCritic,\n",
    "        value_optim_cls: torch.optim.Optimizer,\n",
    "        value_optim_kwargs: dict,\n",
    "        alpha: float = 0.005,\n",
    "        beta: float = 3.0,\n",
    "        gamma: float = 0.99,\n",
    "        tau: float = 0.7,\n",
    "        max_weight=100.0,\n",
    "        total_max_steps: int = 1_000_000,\n",
    "        device: str = \"cpu\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.asymetric_loss = partial(asymetric_loss, tau=tau)\n",
    "        self.device = device\n",
    "        self.max_steps = total_max_steps\n",
    "        self.steps = 0\n",
    "\n",
    "        self._max_weight = max_weight\n",
    "\n",
    "        self.critic = critic.to(device)\n",
    "        self.target_critic = copy.deepcopy(self.critic).to(device)\n",
    "        self.critic_optim = critic_optim_cls(\n",
    "            self.critic.parameters(), **critic_optim_kwargs\n",
    "        )\n",
    "\n",
    "        self.value_critic = value_critic.to(device)\n",
    "        self.value_optim = value_optim_cls(\n",
    "            self.value_critic.parameters(), **value_optim_kwargs\n",
    "        )\n",
    "\n",
    "        self.actor = actor.to(device)\n",
    "        self.actor_optim = actor_optim_cls(\n",
    "            self.actor.parameters(), **actor_optim_kwargs\n",
    "        )\n",
    "        self.actor_scheduler = actor_scheduler(self.actor_optim, total_max_steps)\n",
    "\n",
    "        self._setup_is_called = True\n",
    "\n",
    "    def _update_v(self, states: torch.Tensor, actions: torch.Tensor, logger_info: dict):\n",
    "        with torch.no_grad():\n",
    "            target_q = torch.min(*self.target_critic(states, actions))\n",
    "\n",
    "        value = self.value_critic(states)\n",
    "        advantage = target_q - value\n",
    "        loss = self.asymetric_loss(advantage)\n",
    "        logger_info[\"value_critic_loss\"] = loss.item()\n",
    "        self.value_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.value_optim.step()\n",
    "        return advantage\n",
    "\n",
    "    def _update_q(\n",
    "        self,\n",
    "        states: torch.Tensor,\n",
    "        actions: torch.Tensor,\n",
    "        rewards: torch.Tensor,\n",
    "        dones: torch.Tensor,\n",
    "        next_v: torch.Tensor,\n",
    "        logger_info: dict,\n",
    "    ):\n",
    "        objective = rewards + (1.0 - dones.float()) * next_v.detach() * self.gamma\n",
    "        qs = self.critic(states, actions)\n",
    "        loss = sum(F.mse_loss(q, objective) for q in qs) / len(qs)\n",
    "        logger_info[\"critic_loss\"] = loss.item()\n",
    "        self.critic_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "    def _soft_update(self, target_net, source_net):\n",
    "        for target_parameter, source_parameter in zip(\n",
    "            target_net.parameters(), source_net.parameters()\n",
    "        ):\n",
    "            target_parameter.data.mul_(1 - self.alpha)\n",
    "            target_parameter.data.add_(self.alpha * source_parameter.data)\n",
    "\n",
    "    def _extract_policy(\n",
    "        self,\n",
    "        states: torch.Tensor,\n",
    "        actions: torch.Tensor,\n",
    "        advantage: torch.Tensor,\n",
    "        logger_info: dict,\n",
    "    ):\n",
    "        # And following Brandfonbrener et al. (2021) we clip exponentiated advantages to (−∞, 100].\n",
    "        exp = torch.exp(self.beta * advantage.detach()).clamp(max=self._max_weight)\n",
    "        policy = self.actor(states)\n",
    "        if isinstance(policy, torch.distributions.Distribution):\n",
    "            bc_loss = -policy.log_prob(actions)\n",
    "        elif torch.is_tensor(policy):\n",
    "            bc_loss = torch.sum((policy - actions) ** 2, dim=1)\n",
    "        loss = torch.mean(exp * bc_loss)\n",
    "        logger_info[\"actor_loss\"] = loss.item()\n",
    "        self.actor_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.actor_optim.step()\n",
    "        self.actor_scheduler.step()\n",
    "\n",
    "    def update(self, batch: TensorBatch) -> dict:\n",
    "        if not self._setup_is_called:\n",
    "            raise RuntimeError(\"Setup is not called, cannot proceed with training\")\n",
    "\n",
    "        self.steps += 1\n",
    "        logger_info = {}\n",
    "\n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "        with torch.no_grad():\n",
    "            next_v = self.value_critic(next_states)\n",
    "        advantage = self._update_v(states, actions, logger_info)\n",
    "        self._update_q(\n",
    "            states,\n",
    "            actions,\n",
    "            rewards.squeeze(dim=-1),\n",
    "            dones.squeeze(dim=-1),\n",
    "            next_v,\n",
    "            logger_info,\n",
    "        )\n",
    "        self._extract_policy(states, actions, advantage, logger_info)\n",
    "        self._soft_update(self.target_critic, self.critic)\n",
    "\n",
    "        return logger_info\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def act(self, state: np.ndarray):\n",
    "        return self.actor.act(state, self.device)\n",
    "\n",
    "    def _state_objects(self):\n",
    "        return {\n",
    "            \"actor\": self.actor,\n",
    "            \"critic\": self.critic,\n",
    "            \"target_critic\": self.target_critic,\n",
    "            \"value_critic\": self.value_critic,\n",
    "            \"actor_optim\": self.actor_optim,\n",
    "            \"critic_optim\": self.critic_optim,\n",
    "            \"value_optim\": self.value_optim,\n",
    "            \"actor_scheduler\": self.actor_scheduler,\n",
    "        }\n",
    "\n",
    "    def state_dict(self) -> dict:\n",
    "        state = {\"steps\": self.steps}\n",
    "        objects = self._state_objects()\n",
    "        for key in objects:\n",
    "            state[key] = objects[key].state_dict()\n",
    "        return state\n",
    "\n",
    "    def load_state_dict(self, state: dict) -> None:\n",
    "        self.steps = state[\"steps\"]\n",
    "        objects = self._state_objects()\n",
    "        for key in objects:\n",
    "            objects[key].load_state_dict(state[key])\n",
    "\n",
    "\n",
    "class BaseLogger:\n",
    "    def log(self, data: dict, step: int):\n",
    "        raise NotImplemented\n",
    "\n",
    "\n",
    "class DummyWandbLogger(BaseLogger):\n",
    "    def __init__(self, config: TrainConfig):\n",
    "        wandb.init(\n",
    "            project=config.project,\n",
    "            group=config.group,\n",
    "            name=config.name,\n",
    "            config=asdict(config),\n",
    "        )\n",
    "\n",
    "    def log(self, data: dict, step: int):\n",
    "        wandb.log(data, step=step)\n",
    "\n",
    "\n",
    "class PrintLogger(BaseLogger):\n",
    "    def log(self, data, step: int):\n",
    "        print(f\"Step {step}: {data}\")\n",
    "\n",
    "\n",
    "class EvaluationMixin:\n",
    "    \"\"\"\n",
    "    Подмешивает функционал оценки политики\n",
    "    Для успешной работы должны быть определены:\n",
    "        self._env: gym.Env, среда, в которой работает алгоритм\n",
    "        self._logger: None | BaseLogger, подмешивает функционал логгирования normalized_score\n",
    "        self.steps: int, текущее число шагов но только если self._logger не None\n",
    "        self.eval: func, основной класс должен быть отнаследован от nn.Module\n",
    "        self.train: func, основной класс должен быть отнаследован от nn.Module\n",
    "        self.act, func, выбор следующего действия по состоянию\n",
    "    \"\"\"\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, n_episodes: int, seed: int) -> None:\n",
    "        self.eval()\n",
    "        episode_rewards = []\n",
    "        for _ in range(n_episodes):\n",
    "            state, done = self._env.reset(), False\n",
    "            episode_reward = 0.0\n",
    "            while not done:\n",
    "                state, reward, done, _ = self._env.step(self.act(state))\n",
    "                episode_reward += reward\n",
    "            episode_rewards.append(episode_reward)\n",
    "        self.train()\n",
    "        score = np.asarray(episode_rewards).mean()\n",
    "        normalized_eval_score = self._env.get_normalized_score(score) * 100.0\n",
    "        if self._logger is not None:\n",
    "            self._logger.log(\n",
    "                {\"normalized_score\": normalized_eval_score}, step=self.steps\n",
    "            )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def act(self, state: np.ndarray):\n",
    "        raise NotImplemented\n",
    "\n",
    "\n",
    "class OfflinePretrainMixin(EvaluationMixin):\n",
    "    \"\"\"\n",
    "    Подмешивает функционал Offline претрейна\n",
    "    Добавляет следующие атрибуты:\n",
    "        self._env: gym.Env, конструируется при .setup_env\n",
    "        self._action_dim & self._state_dim: int, конструируется при .setup_env, размерности пространств\n",
    "        self._logger: BaseLogger | None, конструируется при запуске .run_offline\n",
    "        self._replay_buffer: ReplayBuffer, создается при .run_offline\n",
    "    Для успешной работы должны быть определены:\n",
    "        self.act: func, выбирает следующее действие по состоянию\n",
    "        self.update: func, обновляет алгоритм, принимая на вход batch из буфера\n",
    "        self.steps: int, только при наличии логера\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _reward_range(dataset, max_episode_steps: int):\n",
    "        returns, lengths = [], []\n",
    "        ep_ret, ep_len = 0.0, 0\n",
    "        for reward, done in zip(dataset[\"rewards\"], dataset[\"terminals\"]):\n",
    "            ep_ret += float(reward)\n",
    "            ep_len += 1\n",
    "            if done or ep_len == max_episode_steps:\n",
    "                returns.append(ep_ret)\n",
    "                lengths.append(ep_len)\n",
    "                ep_ret, ep_len = 0.0, 0\n",
    "        lengths.append(ep_len)\n",
    "        return min(returns), max(returns)\n",
    "\n",
    "    def _modify_reward(self, dataset, env_name: str, max_episode_steps=1000):\n",
    "        if any(s in env_name for s in (\"halfcheetah\", \"hopper\", \"walker2d\")):\n",
    "            min_ret, max_ret = self._reward_range(dataset, max_episode_steps)\n",
    "            dataset[\"rewards\"] /= max_ret - min_ret\n",
    "            dataset[\"rewards\"] *= max_episode_steps\n",
    "        elif \"antmaze\" in env_name:\n",
    "            dataset[\"rewards\"] -= 1.0\n",
    "\n",
    "    def _preprocess_dataset(\n",
    "        self,\n",
    "        dataset,\n",
    "        env_name: str,\n",
    "        normalize_reward: bool,\n",
    "        std_eps: float,\n",
    "    ) -> tuple[np.ndarray, np.ndarray]:\n",
    "        if normalize_reward:\n",
    "            self._modify_reward(dataset, env_name)\n",
    "        state_mean, state_std = (\n",
    "            dataset[\"observations\"].mean(0),\n",
    "            dataset[\"observations\"].std(0) + std_eps,\n",
    "        )\n",
    "        for key in (\"observations\", \"next_observations\"):\n",
    "            dataset[key] = (dataset[key] - state_mean) / state_std\n",
    "\n",
    "        return state_mean, state_std\n",
    "\n",
    "    def _wrap_env(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        state_mean: np.ndarray | float,\n",
    "        state_std: np.ndarray | float,\n",
    "    ) -> gym.Env:\n",
    "        def normalize_state(state):\n",
    "            return (state - state_mean) / state_std\n",
    "\n",
    "        return gym.wrappers.TransformObservation(env, normalize_state)\n",
    "\n",
    "    def setup_env(self, config: TrainConfig):\n",
    "        self._env = gym.make(config.train.env_name)\n",
    "        self._state_dim = self._env.observation_space.shape[0]\n",
    "        self._action_dim = self._env.action_space.shape[0]\n",
    "        max_action = float(self._env.action_space.high[0])\n",
    "\n",
    "        return self._state_dim, self._action_dim, max_action\n",
    "\n",
    "    def run_offline(\n",
    "        self,\n",
    "        config: TrainConfig,\n",
    "        logger: BaseLogger | None = None,\n",
    "    ):\n",
    "        self._logger = logger\n",
    "\n",
    "        preprocessing_config, evaluation_config, training_config = (\n",
    "            config.preprocess,\n",
    "            config.eval,\n",
    "            config.train,\n",
    "        )\n",
    "\n",
    "        seed = training_config.seed\n",
    "        seed_everything(seed)\n",
    "\n",
    "        dataset = d4rl.qlearning_dataset(self._env)\n",
    "        mu, std = self._preprocess_dataset(\n",
    "            dataset,\n",
    "            training_config.env_name,\n",
    "            **asdict(preprocessing_config),\n",
    "        )\n",
    "        self._env = self._wrap_env(self._env, mu, std)\n",
    "\n",
    "        self._replay_buffer = ReplayBuffer(\n",
    "            self._state_dim,\n",
    "            self._action_dim,\n",
    "            training_config.buffer_size,\n",
    "            self.device,\n",
    "        )\n",
    "        self._replay_buffer.load_d4rl_dataset(dataset)\n",
    "\n",
    "        evaluate_every_n = evaluation_config.evaluate_every_n\n",
    "        eval_episodes = evaluation_config.eval_episodes\n",
    "\n",
    "        for step_n in range(training_config.max_steps):\n",
    "            batch = self._replay_buffer.sample(training_config.batch_size)\n",
    "            logger_info = self.update(batch)\n",
    "            if self._logger is not None:\n",
    "                self._logger.log(logger_info, step=self.steps)\n",
    "\n",
    "            if evaluate_every_n is not None and (step_n + 1) % evaluate_every_n == 0:\n",
    "                self.evaluate(eval_episodes, seed)\n",
    "\n",
    "    def update(self, batch: TensorBatch) -> dict[str]:\n",
    "        raise NotImplemented\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def act(self, state: np.ndarray):\n",
    "        raise NotImplemented\n",
    "\n",
    "\n",
    "class OnlineFineTuneMixin(EvaluationMixin):\n",
    "    \"\"\"\n",
    "    Подмешивает функционал online finetune\n",
    "    Полагается на наличие следующих атрибутов:\n",
    "        self._env: gym.Env\n",
    "        self._logger: BaseLogger | None\n",
    "        self._replay_buffer: ReplayBuffer\n",
    "    Для успешной работы должны быть определены:\n",
    "        self.act: func, выбирает следующее действие по состоянию\n",
    "        self.update: func, обновляет алгоритм, принимая на вход batch из буфера\n",
    "        self.steps: int, только при наличии логера\n",
    "    \"\"\"\n",
    "\n",
    "    def run_online(self, config: TrainConfig, logger: BaseLogger | None = None):\n",
    "        self._logger = logger\n",
    "        evaluation_config, finetuning_config = config.eval, config.finetune\n",
    "\n",
    "        seed = config.train.seed\n",
    "        evaluate_every_n = evaluation_config.evaluate_every_n\n",
    "        eval_episodes = evaluation_config.eval_episodes\n",
    "\n",
    "        state, done = self._env.reset(), False\n",
    "        for step_n in range(finetuning_config.max_steps):\n",
    "            action = self.act(state)\n",
    "            next_state, reward, done, _ = self._env.step(action)\n",
    "            self._replay_buffer.insert(state, action, reward, float(done), next_state)\n",
    "            batch = self._replay_buffer.sample(finetuning_config.batch_size)\n",
    "            logger_info = self.update(batch)\n",
    "            if self._logger is not None:\n",
    "                self._logger.log(logger_info, step=self.steps)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                state, done = self._env.reset(), False\n",
    "\n",
    "            if evaluate_every_n is not None and (step_n + 1) % evaluate_every_n == 0:\n",
    "                self.evaluate(eval_episodes, seed)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def act(self, state: np.ndarray):\n",
    "        raise NotImplemented\n",
    "\n",
    "\n",
    "class IQLOfflineOnline(IQL, OnlineFineTuneMixin, OfflinePretrainMixin):\n",
    "    ...\n",
    "\n",
    "\n",
    "def train(yaml_path: str = \"\"):\n",
    "    if not yaml_path:\n",
    "        config = TrainConfig()\n",
    "    else:\n",
    "        with open(yaml_path) as f:\n",
    "            options = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "            config = from_dict(TrainConfig, options)\n",
    "\n",
    "    iql_trainer = IQLOfflineOnline()\n",
    "    state_dim, action_dim, max_action = iql_trainer.setup_env(config)\n",
    "    iql_trainer.setup(\n",
    "        actor=GaussianPolicy(state_dim, action_dim, max_action=max_action),\n",
    "        actor_optim_cls=torch.optim.Adam,\n",
    "        actor_optim_kwargs={\"lr\": 3e-4},\n",
    "        critic=Critic(state_dim, action_dim),\n",
    "        critic_optim_cls=torch.optim.Adam,\n",
    "        critic_optim_kwargs={\"lr\": 3e-4},\n",
    "        value_critic=ValueCritic(state_dim),\n",
    "        value_optim_cls=torch.optim.Adam,\n",
    "        value_optim_kwargs={\"lr\": 3e-4},\n",
    "        actor_scheduler=torch.optim.lr_scheduler.CosineAnnealingLR,\n",
    "        total_max_steps=config.train.max_steps,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "        **asdict(config.algorithm),\n",
    "    )\n",
    "    wandb_logger = DummyWandbLogger(config)\n",
    "    iql_trainer.run_offline(config, wandb_logger)\n",
    "    if config.train.save_path:\n",
    "        torch.save(iql_trainer.state_dict(), config.train.save_path)\n",
    "    iql_trainer.run_online(config, wandb_logger)\n",
    "    if config.finetune.save_path:\n",
    "        torch.save(iql_trainer.state_dict(), config.finetune.save_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}